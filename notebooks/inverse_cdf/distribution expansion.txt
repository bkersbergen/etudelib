The technique of increasing the number of items in a distribution while maintaining the shape of the distribution is called "distribution expansion" or "distribution extrapolation". It is a common technique in machine learning and data science for generating synthetic data, testing algorithms, and other applications where it is necessary to work with data that has a particular distribution or statistical properties.



Suppose we have a set of n items, and we observe the frequency of each item over a period of time. Let f_i be the frequency of item i, and let p_i = f_i / \sum_{j=1}^n f_j be the probability of item i.

We assume that the popularity distribution of the items follows a Zipfian distribution, which has the following probability mass function:

P(i) = c / i^k
where c is a normalization constant and k is a parameter that controls the shape of the distribution. The Zipfian distribution is characterized by a heavy tail, where a small number of items have high probabilities, and a large number of items have low probabilities.

To increase the number of items while maintaining the shape of the distribution, we can use the following algorithm:

Fit a power law curve to the observed frequencies of the items, using a tool such as the powerlaw library in Python. The power law curve has the following form:

f(i) = c * i^(-k)

where c and k are parameters to be estimated.

Use the estimated parameters to generate probabilities for new items. To do this, we first choose a set of m new item IDs to add to the set of n observed items. We can choose these IDs randomly, or based on some other criteria. For each new item ID j, we compute its probability as:

p_j = c / (n + j)^k

where n is the number of observed items.

Combine the observed item probabilities p_i and the new item probabilities p_j to obtain a probability distribution over all n + m items.

Use linear interpolation to create a mapping between the cumulative probabilities and the item IDs, as in the previous example. This allows us to efficiently sample items from the distribution, while still maintaining the shape of the distribution.

By generating new items in this way, we can increase the size of the item set while still maintaining the heavy-tailed shape of the distribution, which is often seen in real-world data.



import numpy as np
from scipy import interpolate
from powerlaw import Fit

# Set up the observed item frequencies and probabilities
f_obs = np.array([100, 50, 30, 20, 10])
p_obs = f_obs / np.sum(f_obs)

# Fit a power-law curve to the observed frequencies
fit = Fit(f_obs, xmin=1)
c, k = fit.power_law.alpha, fit.power_law.xmin

# Generate probabilities for new items
m = 5  # number of new items
n = len(f_obs)  # number of observed items
new_ids = np.arange(n + 1, n + m + 1)  # generate new item IDs
p_new = c / np.power(n + new_ids, k)  # compute new item probabilities

# Combine the observed and new item probabilities
p_all = np.concatenate([p_obs, p_new])

# Use linear interpolation to create a mapping between the cumulative probabilities and the item IDs
cdf = np.cumsum(p_all)
inv_cdf = interpolate.interp1d(cdf, np.arange(len(p_all)), kind='linear')

# Sample new items from the distribution
samples = []
for i in range(10):
    u = np.random.uniform()
    idx = int(inv_cdf(u))
    samples.append(idx)

# Print the samples and their probabilities
for i in samples:
    if i < n:
        print(f"Sampled observed item {i+1} with probability {p_all[i]}")
    else:
        print(f"Sampled new item {i-n+1} with probability {p_all[i]}")
        


In this example, we start with a set of observed item frequencies f_obs and probabilities p_obs. We then fit a power-law curve to the observed frequencies using the powerlaw library, and use the estimated parameters to generate probabilities for a set of new items. We combine the observed and new item probabilities, and use linear interpolation to create a mapping between the cumulative probabilities and the item IDs. Finally, we sample new items from the distribution using the inverse mapping, and print out the samples and their probabilities.

        